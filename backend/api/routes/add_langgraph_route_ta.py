from assistant_stream import create_run, RunController
from assistant_stream.serialization import DataStreamResponse
from langchain_core.messages import (
   HumanMessage,
   AIMessageChunk,
   AIMessage,
   ToolMessage,
   SystemMessage,
   BaseMessage,
)
from fastapi import FastAPI, Header
from pydantic import BaseModel
from typing import List, Literal, Union, Optional, Any
from ..utils.deps import get_current_user
from ..models.user import UserInDB
from fastapi import Depends, HTTPException, status
from ..database.mongodb import MongoDB
from bson import ObjectId
from langfuse.callback import CallbackHandler
from langfuse.decorators import observe
from langfuse import Langfuse
langfuse = Langfuse()


class LanguageModelTextPart(BaseModel):
   type: Literal["text"]
   text: str
   providerMetadata: Optional[Any] = None




class LanguageModelImagePart(BaseModel):
   type: Literal["image"]
   image: str  # Will handle URL or base64 string
   mimeType: Optional[str] = None
   providerMetadata: Optional[Any] = None




class LanguageModelFilePart(BaseModel):
   type: Literal["file"]
   data: str  # URL or base64 string
   mimeType: str
   providerMetadata: Optional[Any] = None




class LanguageModelToolCallPart(BaseModel):
   type: Literal["tool-call"]
   toolCallId: str
   toolName: str
   args: Any
   providerMetadata: Optional[Any] = None




class LanguageModelToolResultContentPart(BaseModel):
   type: Literal["text", "image"]
   text: Optional[str] = None
   data: Optional[str] = None
   mimeType: Optional[str] = None




class LanguageModelToolResultPart(BaseModel):
   type: Literal["tool-result"]
   toolCallId: str
   toolName: str
   result: Any
   isError: Optional[bool] = None
   content: Optional[List[LanguageModelToolResultContentPart]] = None
   providerMetadata: Optional[Any] = None




class LanguageModelSystemMessage(BaseModel):
   role: Literal["system"]
   content: str




class LanguageModelUserMessage(BaseModel):
   role: Literal["user"]
   content: List[
       Union[LanguageModelTextPart, LanguageModelImagePart, LanguageModelFilePart]
   ]




class LanguageModelAssistantMessage(BaseModel):
   role: Literal["assistant"]
   content: List[Union[LanguageModelTextPart, LanguageModelToolCallPart]]




class LanguageModelToolMessage(BaseModel):
   role: Literal["tool"]
   content: List[LanguageModelToolResultPart]




LanguageModelV1Message = Union[
   LanguageModelSystemMessage,
   LanguageModelUserMessage,
   LanguageModelAssistantMessage,
   LanguageModelToolMessage,
]




def convert_to_langchain_messages(
   messages: List[LanguageModelV1Message],
) -> List[BaseMessage]:
   result = []


   for msg in messages:
       if msg.role == "system":
           result.append(SystemMessage(content=msg.content))


       elif msg.role == "user":
           content = []
           for p in msg.content:
               if isinstance(p, LanguageModelTextPart):
                   content.append({"type": "text", "text": p.text})
               elif isinstance(p, LanguageModelImagePart):
                   content.append({"type": "image_url", "image_url": p.image})
           result.append(HumanMessage(content=content))


       elif msg.role == "assistant":
           # Handle both text and tool calls
           text_parts = [
               p for p in msg.content if isinstance(p, LanguageModelTextPart)
           ]
           text_content = " ".join(p.text for p in text_parts)
           tool_calls = [
               {
                   "id": p.toolCallId,
                   "name": p.toolName,
                   "args": p.args,
               }
               for p in msg.content
               if isinstance(p, LanguageModelToolCallPart)
           ]
           print(tool_calls)
           result.append(AIMessage(content=text_content, tool_calls=tool_calls))


       elif msg.role == "tool":
           for tool_result in msg.content:
               result.append(
                   ToolMessage(
                       content=str(tool_result.result),
                       tool_call_id=tool_result.toolCallId,
                   )
               )


   return result




class FrontendToolCall(BaseModel):
   name: str
   description: Optional[str] = None
   parameters: dict[str, Any]




class ChatRequest(BaseModel):
   system: Optional[str] = ""
   tools: Optional[List[FrontendToolCall]] = []
   messages: List[LanguageModelV1Message]



   

def add_langgraph_route(app: FastAPI, graph, path: str, current_user: UserInDB = Depends(get_current_user)):

    # ... (SYSTEM_MESSAGE definition)
    SYSTEM_MESSAGE = """
        # AI Teaching Assistant (TA) Chatbot for Students


        You are a knowledgeable and helpful AI Teaching Assistant built to support students in mastering the course material. Your responses are grounded in the official course textbooks and academic materials provided by the instructor. You are not a general-purpose assistant — your sole focus is helping students succeed in this course.


        ## Your Capabilities


        1. **get_textbook_context**: This is your primary tool. It performs semantic search over the combined course textbook content stored in MongoDB using OpenAI embeddings. You should always try to answer using this tool first.


        2. **fetch**: If a relevant concept is not covered in the textbook, responsibly retrieve it from reliable sources on the internet and cite the source clearly. Additionally, if the user provides a URL in their prompt, you may use `fetch` to access and extract information from that URL, especially if the user asks about its relevance, usefulness, or content.


        ## How You Operate


        1. You begin by using `get_textbook_context` to search the course materials for relevant passages based on the student’s question.


        2. If the textbook does not contain the answer and the question is still within the course scope, use `fetch` to retrieve accurate, cited information from trusted academic or technical sources. If the user includes a URL in their prompt, you may also use `fetch` to access that link and respond based on its content.


        3. You only answer questions that are within the course scope. If a student asks something unrelated, you clearly explain that you're limited to course material.


        ## Interaction Guidelines


        - Be friendly, concise, and academically helpful. 
        - Emphasize clarity and understanding. 
        - If useful, suggest the student review similar textbook sections for deeper understanding. 
        - Always cite your source if using content retrieved from outside the textbook. 
        - If the answer is not in the textbook and you fetch online, clarify that the information was not found in the textbook and provide the source. 
        - Never speculate or hallucinate information — rely strictly on the textbook content or verified external sources.


        ## Special Instructions


        - Always use `get_textbook_context` first for any academic query. 
        - If textbook information is insufficient but the question is within scope, use `fetch` to gather trusted external content and cite it. 
        - If a question is outside the scope of the course, respond politely: 
        _"I'm designed to help with topics from the course only; this question appears to be outside that scope."_ 


        Your goal is to help students understand the material, develop confidence, and succeed — using only the resources provided in the course and reliable academic sources when needed.
        """

    @app.get(f"{path}/history")
    async def get_chat_history(x_chat_id: Optional[str] = Header(None, alias="X-Chat-ID"), current_user: dict = Depends(get_current_user)):
        if not x_chat_id:
            return {"history": []}

        db = MongoDB.get_db()
        chat_session = db.chat_sessions.find_one(
            {"chat_id": x_chat_id, "user_id": ObjectId(current_user.id)})

        if chat_session:
            return {"history": chat_session.get("messages", [])}
        return {"history": []}
    
    @app.get(f"{path}/sessions")
    async def get_chat_sessions(current_user: dict = Depends(get_current_user)):
        db = MongoDB.get_db()
        # Find all sessions for the user, sorted by most recent
        sessions_cursor = db.chat_sessions.find(
            {"user_id": ObjectId(current_user.id)},
            # Only retrieve the chat_id and the first message for the title
            {"chat_id": 1, "messages": {"$slice": 1}}
        ).sort("_id", -1)

        sessions = []
        # for session in await sessions_cursor.to_list(length=100):
        for session in sessions_cursor.limit(100):
            first_message_content = "New Chat"
            # Safely get the text from the first message to use as a title
            if session.get("messages"):
                try:
                    first_message_content = session["messages"][0]["content"][0]["text"]
                except (IndexError, KeyError):
                    # If message is not text, keep the default title
                    pass

            sessions.append({
                "chat_id": session["chat_id"],
                # Truncate for clean display
                "title": first_message_content[:50]
            })

        return {"sessions": sessions}

    async def chat_completions(request: ChatRequest, x_chat_id: Optional[str] = Header(None, alias="X-Chat-ID"), current_user: dict = Depends(get_current_user)):
        db = MongoDB.get_db()
        user = db.users.find_one({"_id": ObjectId(current_user.id)})

        if user["requests_used"] >= user["requests_limit"]:
            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail="Request limit exceeded"
            )

        db.users.update_one(
            {"_id": ObjectId(current_user.id)},
            {"$inc": {"requests_used": 1}}
        )

        inputs = convert_to_langchain_messages(request.messages)
        # all_messages = inputs
        new_message_input = inputs[-1:]
        # system_msg = SystemMessage(content=SYSTEM_MESSAGE)

        # Load chat history
        # chat_session = db.chat_sessions.find_one(
        #     {"chat_id": x_chat_id, "user_id": ObjectId(current_user.id)})
        # history_from_db = []
        # if chat_session:
        #     history_from_db = chat_session.get("messages", [])

        # history_messages = []
        # for msg in history_from_db:
        #     if msg['role'] == 'user':
        #         history_messages.append(LanguageModelUserMessage(**msg))
        #     elif msg['role'] == 'assistant':
        #         history_messages.append(LanguageModelAssistantMessage(**msg))

        # history = convert_to_langchain_messages(history_messages)

        # all_messages = history + inputs

        accumulated_content = ""
        tool_calls = {}

        trace = None
        if current_user.enable_logging:
            trace = langfuse.trace(
                user_id=current_user.email,
                session_id=x_chat_id
            )
            trace.update(
                input=inputs[-1].content[0]['text'],
            )

        config = {
            "configurable": {
                "thread_id": x_chat_id,
                "system": SYSTEM_MESSAGE,
                "frontend_tools": request.tools,
                "metadata": {
                    "langfuse_session_id": x_chat_id,
                    "current_user": current_user.email,
                    "current_id": current_user.id
                },
            }
        }

        async def run(controller: RunController):
            tool_calls = {}
            tool_calls_by_idx = {}
            nonlocal accumulated_content

            async for msg, metadata in graph.astream(
                {"messages": new_message_input},
                config=config,
                stream_mode="messages"
            ):
                if isinstance(msg, ToolMessage):
                    tool_controller = tool_calls.get(msg.tool_call_id)
                    if tool_controller is None:
                        tool_controller = await controller.add_tool_call("MCP", msg.tool_call_id)
                        tool_calls[msg.tool_call_id] = tool_controller

                    tool_controller.set_result(msg.content)

                if isinstance(msg, AIMessageChunk) or isinstance(msg, AIMessage):
                    if msg.content:
                        accumulated_content += msg.content
                        controller.append_text(msg.content)

                    for chunk in msg.tool_call_chunks:
                        if not chunk["index"] in tool_calls_by_idx:
                            tool_controller = await controller.add_tool_call(
                                chunk["name"], chunk["id"]
                            )
                            tool_calls_by_idx[chunk["index"]] = tool_controller
                            tool_calls[chunk["id"]] = tool_controller
                        else:
                            tool_controller = tool_calls_by_idx[chunk["index"]]

                        tool_controller.append_args_text(chunk["args"])

            if x_chat_id:
                thread_state = graph.get_state(config)
                final_messages = thread_state.values['messages']

                messages_to_store = []
                for msg in final_messages:
                    role = ""
                    content = []
                    if isinstance(msg, HumanMessage):
                        role = "user"
                        for part in msg.content:
                            if part['type'] == 'text':
                                content.append(LanguageModelTextPart(
                                    type='text', text=part['text']).dict())
                    elif isinstance(msg, AIMessage):
                        role = "assistant"
                        if msg.content:
                            content.append(LanguageModelTextPart(
                                type='text', text=msg.content).dict())
                        if msg.tool_calls:
                            for tc in msg.tool_calls:
                                content.append(LanguageModelToolCallPart(
                                    type='tool-call', toolCallId=tc['id'], toolName=tc['name'], args=tc['args']).dict())

                    if role:
                        messages_to_store.append(
                            {"role": role, "content": content})

                db.chat_sessions.update_one(
                    {"chat_id": x_chat_id,
                     "user_id": ObjectId(current_user.id)},
                    {"$set": {"messages": messages_to_store}},
                    upsert=True
                )

            if trace is not None:
                trace.update(
                    output=accumulated_content
                )

        return DataStreamResponse(create_run(run))

    app.add_api_route(path, chat_completions, methods=["POST"])
